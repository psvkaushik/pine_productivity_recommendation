{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MTL with kmeans clutering and predict the nearest cluster for a sample and then run that specific model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expt 1 - No clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and inspect dataset\n",
    "df1 = pd.read_csv(\"../../data/tr_data.csv\")\n",
    "df2 = pd.read_csv(\"../../data/te_data.csv\")\n",
    "df = pd.concat([df1,df2],axis=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# === Load train and test data separately ===\n",
    "train_df = pd.read_csv(\"../../data/tr_data.csv\")\n",
    "test_df = pd.read_csv(\"../../data/te_data.csv\")    # Replace with your actual path\n",
    "\n",
    "# === Drop irrelevant columns ===\n",
    "drop_cols = ['Unnamed: 0', 'TestId', 'date_initial', 'date_final', 'Feature','longitute','latitude','env']\n",
    "target_class_col = 'Specie'\n",
    "target_reg_col = 'Productivity (y)'\n",
    "\n",
    "train_df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "test_df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "\n",
    "# === Encode species labels ===\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['species_encoded'] = label_encoder.fit_transform(train_df[target_class_col])\n",
    "test_df['species_encoded'] = label_encoder.transform(test_df[target_class_col])\n",
    "\n",
    "# === Correlation Analysis on Train Set ===\n",
    "# Compute correlation matrix\n",
    "# Select only numeric columns for correlation\n",
    "numeric_train_df = train_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = numeric_train_df.corr()\n",
    "\n",
    "# Find features most correlated with Productivity (y)\n",
    "correlation_threshold = 0.3  # you can adjust\n",
    "strong_corr_features = corr_matrix[target_reg_col].abs()\n",
    "selected_features = strong_corr_features[strong_corr_features > correlation_threshold].index.tolist()\n",
    "\n",
    "# Remove target columns themselves\n",
    "selected_features = [f for f in selected_features if f not in [target_reg_col, target_class_col, 'species_encoded']]\n",
    "\n",
    "print(f\"Selected Features after Correlation Analysis: {selected_features}\")\n",
    "\n",
    "\n",
    "# === Prepare Feature and Target Arrays ===\n",
    "\n",
    "# Train Features and Targets\n",
    "X_train = train_df[selected_features].fillna(0).values\n",
    "y_species_train = train_df['species_encoded'].values\n",
    "y_prod_train = train_df[target_reg_col].values\n",
    "\n",
    "# Test Features and Targets\n",
    "X_test = test_df[selected_features].fillna(0).values\n",
    "y_species_test = test_df['species_encoded'].values\n",
    "y_prod_test = test_df[target_reg_col].values\n",
    "\n",
    "# === Standardize Features based on Train ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Now you have:\n",
    "# - X_train_scaled, y_species_train, y_prod_train\n",
    "# - X_test_scaled, y_species_test, y_prod_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PyTorch dataset\n",
    "class PineDataset(Dataset):\n",
    "    def __init__(self, X, y_species, y_productivity):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_species = torch.tensor(y_species, dtype=torch.long)\n",
    "        self.y_productivity = torch.tensor(y_productivity, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_species[idx], self.y_productivity[idx]\n",
    "\n",
    "train_dataset = PineDataset(X_train, y_species_train, y_prod_train)\n",
    "test_dataset = PineDataset(X_test, y_species_test, y_prod_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeciesProductivityModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, use_uncertainty=True,alpha =1):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "        self.regressor = nn.Linear(64, 1)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        # For uncertainty weighting\n",
    "        self.use_uncertainty = use_uncertainty\n",
    "        if use_uncertainty:\n",
    "            self.log_sigma_cls = nn.Parameter(torch.tensor(0.0))\n",
    "            self.log_sigma_reg = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        class_logits = self.classifier(x)\n",
    "        regression = self.regressor(x).squeeze(1)\n",
    "        return class_logits, regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_weighted_loss(loss_cls, loss_reg, model):\n",
    "    sigma_c = model.log_sigma_cls\n",
    "    sigma_r = model.log_sigma_reg\n",
    "\n",
    "    loss = (1 / (2 * torch.exp(sigma_c) ** 2)) * loss_cls + sigma_c\n",
    "    loss += (1 / (2 * torch.exp(sigma_r) ** 2)) * loss_reg + sigma_r\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_species_train))\n",
    "model = SpeciesProductivityModel(input_dim, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_class = nn.CrossEntropyLoss()\n",
    "loss_regression = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for x_batch, y_cls, y_reg in train_loader:\n",
    "        logits, preds = model(x_batch)\n",
    "        loss_cls = loss_class(logits, y_cls)\n",
    "        loss_reg = loss_regression(preds, y_reg)\n",
    "      \n",
    "        loss = uncertainty_weighted_loss(loss_cls, loss_reg, model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds_cls, all_true_cls = [], []\n",
    "all_preds_reg, all_true_reg = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_cls, y_reg in test_loader:\n",
    "        logits, preds = model(x_batch)\n",
    "        y_pred_cls = torch.argmax(logits, dim=1)\n",
    "        all_preds_cls.extend(y_pred_cls.numpy())\n",
    "        all_true_cls.extend(y_cls.numpy())\n",
    "        all_preds_reg.extend(preds.numpy())\n",
    "        all_true_reg.extend(y_reg.numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_true_cls, all_preds_cls)\n",
    "rmse = np.sqrt(mean_squared_error(all_true_reg, all_preds_reg))\n",
    "r2 = r2_score(all_true_reg, all_preds_reg)\n",
    "\n",
    "print(f\"✅ Species Classification Accuracy: {accuracy:.4f}\")\n",
    "print(f\"✅ Productivity RMSE: {rmse:.4f}\")\n",
    "print(f\"✅ Productivity R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 3\n",
    "all_top3_preds = []\n",
    "all_true_species = []\n",
    "all_pred_productivity = []\n",
    "all_true_productivity = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_cls, y_reg in test_loader:\n",
    "        logits, pred_reg = model(x_batch)\n",
    "\n",
    "        # Get top-3 class predictions\n",
    "        top3 = torch.topk(logits, k=topk, dim=1).indices  # (batch_size, 3)\n",
    "\n",
    "        all_top3_preds.extend(top3.numpy())\n",
    "        all_true_species.extend(y_cls.numpy())\n",
    "        all_pred_productivity.extend(pred_reg.numpy())\n",
    "        all_true_productivity.extend(y_reg.numpy())\n",
    "\n",
    "# ✅ Compute Top-3 Accuracy\n",
    "correct_top3 = 0\n",
    "for true, top3 in zip(all_true_species, all_top3_preds):\n",
    "    if true in top3:\n",
    "        correct_top3 += 1\n",
    "top3_accuracy = correct_top3 / len(all_true_species)\n",
    "print(f\"✅ Top-3 Species Accuracy: {top3_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expt 2 - Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tiles/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# === Load and clean data ===\n",
    "train_df = pd.read_csv(\"../../data/tr_data.csv\")\n",
    "test_df = pd.read_csv(\"../../data/te_data.csv\")\n",
    "\n",
    "drop_cols = ['Unnamed: 0', 'TestId', 'date_initial', 'date_final', 'Feature','longitute','latitude']\n",
    "train_df = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "test_df = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Encode species\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['species_encoded'] = label_encoder.fit_transform(train_df['Specie'])\n",
    "test_df['species_encoded'] = label_encoder.transform(test_df['Specie'])\n",
    "\n",
    "# Keep numeric and clean\n",
    "train_df = train_df.select_dtypes(include=[np.number]).dropna()\n",
    "test_df = test_df.select_dtypes(include=[np.number]).dropna()\n",
    "\n",
    "X_train = train_df.drop(columns=['Productivity (y)', 'species_encoded'])\n",
    "y_train_cls = train_df['species_encoded']\n",
    "y_train_reg = train_df['Productivity (y)']\n",
    "\n",
    "X_test = test_df.drop(columns=['Productivity (y)', 'species_encoded'])\n",
    "y_test_cls = test_df['species_encoded']\n",
    "y_test_reg = test_df['Productivity (y)']\n",
    "\n",
    "# === Standardize ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Cluster with KMeans ===\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "train_clusters = kmeans.fit_predict(X_train_scaled)\n",
    "\n",
    "# Centroid classifier for test-time assignment\n",
    "centroid_classifier = NearestCentroid()\n",
    "centroid_classifier.fit(X_train_scaled, train_clusters)\n",
    "\n",
    "# === Torch dataset ===\n",
    "class PineDataset(Dataset):\n",
    "    def __init__(self, X, y_cls, y_reg):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y_cls = torch.tensor(y_cls.values, dtype=torch.long)\n",
    "        self.y_reg = torch.tensor(y_reg.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y_cls[idx], self.y_reg[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterMTLNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "        self.regressor = nn.Linear(128, 1)\n",
    "\n",
    "        # Uncertainty weighting params (log sigmas)\n",
    "        self.log_sigma_cls = nn.Parameter(torch.tensor(0.0))\n",
    "        self.log_sigma_reg = nn.Parameter(torch.tensor(0.0))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        class_logits = self.classifier(shared)\n",
    "        reg_output = self.regressor(shared).squeeze(1)\n",
    "        return class_logits, reg_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def souncertainty_weighted_loss(loss_cls, loss_reg, model):\n",
    "    sigma_cls = model.log_sigma_cls\n",
    "    sigma_reg = model.log_sigma_reg\n",
    "    alpha = model.alpha\n",
    "\n",
    "    loss = (0.5 * loss_cls / torch.exp(2 * sigma_cls)) + alpha * sigma_cls\n",
    "    loss += (0.5 * loss_reg / torch.exp(2 * sigma_reg)) + alpha * sigma_reg\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KMeans better MTL Accuracy (Species): 0.7737\n",
      "✅ KMeans better MTL R² (Productivity): 0.7476\n"
     ]
    }
   ],
   "source": [
    "cluster_models = {}\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = y_train_cls.nunique()\n",
    "\n",
    "for cluster_id in np.unique(train_clusters):\n",
    "    idx = train_clusters == cluster_id\n",
    "    X_c = X_train[idx]\n",
    "    y_c_cls = y_train_cls[idx]\n",
    "    y_c_reg = y_train_reg[idx]\n",
    "\n",
    "    model = BetterMTLNet(input_dim, num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_cls = nn.CrossEntropyLoss()\n",
    "    loss_reg = nn.MSELoss()\n",
    "\n",
    "    dataset = PineDataset(X_c, y_c_cls, y_c_reg)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for xb, yb_cls, yb_reg in loader:\n",
    "            logits, reg_out = model(xb)\n",
    "            l_cls = loss_cls(logits, yb_cls)\n",
    "            l_reg = loss_reg(reg_out, yb_reg)\n",
    "            loss = souncertainty_weighted_loss(l_cls, l_reg, model)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    cluster_models[cluster_id] = model\n",
    "\n",
    "# === Predict test samples using nearest cluster ===\n",
    "y_cls_preds = []\n",
    "y_reg_preds = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    x_raw = X_test.iloc[i:i+1]\n",
    "    x_tensor = torch.tensor(x_raw.values, dtype=torch.float32)\n",
    "    cluster_id = centroid_classifier.predict([X_test_scaled[i]])[0]\n",
    "\n",
    "    model = cluster_models[cluster_id]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, reg_out = model(x_tensor)\n",
    "        pred_cls = torch.argmax(logits, dim=1).item()\n",
    "        pred_reg = reg_out.item()\n",
    "\n",
    "    y_cls_preds.append(pred_cls)\n",
    "    y_reg_preds.append(pred_reg)\n",
    "\n",
    "# === Evaluation ===\n",
    "acc = accuracy_score(y_test_cls, y_cls_preds)\n",
    "r2 = r2_score(y_test_reg, y_reg_preds)\n",
    "\n",
    "print(f\"✅ KMeans better MTL Accuracy (Species): {acc:.4f}\")\n",
    "print(f\"✅ KMeans better MTL R² (Productivity): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN + MTL code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expt 1 - Without clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 10747.1958\n",
      "Epoch 2 | Loss: 4053.5753\n",
      "Epoch 3 | Loss: 3356.1904\n",
      "Epoch 4 | Loss: 3077.4677\n",
      "Epoch 5 | Loss: 2916.1092\n",
      "Epoch 6 | Loss: 2849.1620\n",
      "Epoch 7 | Loss: 2783.2814\n",
      "Epoch 8 | Loss: 2729.4028\n",
      "Epoch 9 | Loss: 2669.1367\n",
      "Epoch 10 | Loss: 2633.7491\n",
      "Epoch 11 | Loss: 2587.9395\n",
      "Epoch 12 | Loss: 2546.1974\n",
      "Epoch 13 | Loss: 2519.1892\n",
      "Epoch 14 | Loss: 2486.9600\n",
      "Epoch 15 | Loss: 2459.4866\n",
      "Epoch 16 | Loss: 2422.2980\n",
      "Epoch 17 | Loss: 2392.1825\n",
      "Epoch 18 | Loss: 2364.0668\n",
      "Epoch 19 | Loss: 2343.7454\n",
      "Epoch 20 | Loss: 2320.1114\n",
      "✅ Accuracy (Species): 0.6427745664739885\n",
      "✅ R² (Productivity): 0.7100981956200969\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# === Load and preprocess ===\n",
    "train_df = pd.read_csv(\"../../data/tr_data.csv\")\n",
    "test_df = pd.read_csv(\"../../data/te_data.csv\")\n",
    "\n",
    "drop_cols = ['Unnamed: 0', 'TestId', 'date_initial', 'date_final', 'Feature','latitude','longitute','env']\n",
    "train_df = train_df.drop(columns=drop_cols, errors='ignore').dropna()\n",
    "test_df = test_df.drop(columns=drop_cols, errors='ignore').dropna()\n",
    "\n",
    "# Label encode species\n",
    "le = LabelEncoder()\n",
    "train_df['species_encoded'] = le.fit_transform(train_df['Specie'])\n",
    "test_df['species_encoded'] = le.transform(test_df['Specie'])\n",
    "\n",
    "# Features and targets\n",
    "X_train = train_df.drop(columns=['Specie', 'species_encoded', 'Productivity (y)'])\n",
    "y_cls_train = train_df['species_encoded']\n",
    "y_reg_train = train_df['Productivity (y)']\n",
    "\n",
    "X_test = test_df.drop(columns=['Specie', 'species_encoded', 'Productivity (y)'])\n",
    "y_cls_test = test_df['species_encoded']\n",
    "y_reg_test = test_df['Productivity (y)']\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === PyTorch Dataset ===\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y_cls, y_reg):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_cls = torch.tensor(y_cls.values, dtype=torch.long)\n",
    "        self.y_reg = torch.tensor(y_reg.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y_cls[idx], self.y_reg[idx]\n",
    "\n",
    "train_loader = DataLoader(TabularDataset(X_train_scaled, y_cls_train, y_reg_train), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TabularDataset(X_test_scaled, y_cls_test, y_reg_test), batch_size=1, shuffle=False)\n",
    "\n",
    "# === CNN-based MTL Model ===\n",
    "class CNNMTLNetWithSOUW(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "        self.regressor = nn.Linear(64, 1)\n",
    "        self.log_sigma_cls = nn.Parameter(torch.tensor(0.0))\n",
    "        self.log_sigma_reg = nn.Parameter(torch.tensor(0.0))\n",
    "        self.alpha = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # shape [B, 1, F]\n",
    "        x = self.conv(x).squeeze(2)  # shape [B, 32]\n",
    "        x = self.shared(x)\n",
    "        return self.classifier(x), self.regressor(x).squeeze(1)\n",
    "\n",
    "# === SOUW Loss Function ===\n",
    "def souncertainty_weighted_loss(loss_cls, loss_reg, model):\n",
    "    sigma_cls = model.log_sigma_cls\n",
    "    sigma_reg = model.log_sigma_reg\n",
    "    alpha = model.alpha\n",
    "    return (\n",
    "        0.5 * loss_cls / torch.exp(2 * sigma_cls) + alpha * sigma_cls +\n",
    "        0.5 * loss_reg / torch.exp(2 * sigma_reg) + alpha * sigma_reg\n",
    "    )\n",
    "\n",
    "# === Training ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNMTLNetWithSOUW(input_dim=X_train.shape[1], num_classes=y_cls_train.nunique()).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_cls_fn = nn.CrossEntropyLoss()\n",
    "loss_reg_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb_cls, yb_reg in train_loader:\n",
    "        xb, yb_cls, yb_reg = xb.to(device), yb_cls.to(device), yb_reg.to(device)\n",
    "        logits, reg_out = model(xb)\n",
    "        loss_cls = loss_cls_fn(logits, yb_cls)\n",
    "        loss_reg = loss_reg_fn(reg_out, yb_reg)\n",
    "        loss = souncertainty_weighted_loss(loss_cls, loss_reg, model)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "# === Evaluation ===\n",
    "model.eval()\n",
    "y_cls_preds, y_reg_preds = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _, _ in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits, reg_out = model(xb)\n",
    "        y_cls_preds.append(torch.argmax(logits, dim=1).item())\n",
    "        y_reg_preds.append(reg_out.item())\n",
    "\n",
    "print(\"✅ Accuracy (Species):\", accuracy_score(y_cls_test, y_cls_preds))\n",
    "print(\"✅ R² (Productivity):\", r2_score(y_reg_test, y_reg_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expt 2 - Using Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cluster-CNN MTL Accuracy (Species): 0.6401\n",
      "✅ Cluster-CNN MTL R² (Productivity): 0.6201\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Load train/test ===\n",
    "train_df = pd.read_csv(\"../../data/tr_data.csv\")\n",
    "test_df = pd.read_csv(\"../../data/te_data.csv\")\n",
    "\n",
    "drop_cols = ['Unnamed: 0', 'TestId', 'date_initial', 'date_final', 'Feature','latitude','longitute','env']\n",
    "train_df = train_df.drop(columns=drop_cols, errors='ignore').dropna()\n",
    "test_df = test_df.drop(columns=drop_cols, errors='ignore').dropna()\n",
    "\n",
    "# Encode species\n",
    "le = LabelEncoder()\n",
    "train_df['species_encoded'] = le.fit_transform(train_df['Specie'])\n",
    "test_df['species_encoded'] = le.transform(test_df['Specie'])\n",
    "\n",
    "# Split features and targets\n",
    "X_train_env = train_df.drop(columns=['Specie', 'species_encoded', 'Productivity (y)'])  # environmental vars\n",
    "X_test_env = test_df.drop(columns=['Specie', 'species_encoded', 'Productivity (y)'])\n",
    "\n",
    "y_cls_train = train_df['species_encoded']\n",
    "y_reg_train = train_df['Productivity (y)']\n",
    "y_cls_test = test_df['species_encoded']\n",
    "y_reg_test = test_df['Productivity (y)']\n",
    "\n",
    "# === Normalize environmental features ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_env)\n",
    "X_test_scaled = scaler.transform(X_test_env)\n",
    "\n",
    "# === Clustering ===\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "train_clusters = kmeans.fit_predict(X_train_scaled)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# === Cluster-wise data storage ===\n",
    "cluster_models = {}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y_cls, y_reg):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_cls = torch.tensor(y_cls.values, dtype=torch.long)\n",
    "        self.y_reg = torch.tensor(y_reg.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y_cls[idx], self.y_reg[idx]\n",
    "\n",
    "# CNN-MTL model\n",
    "class CNNMTLNetWithSOUW(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "        self.regressor = nn.Linear(64, 1)\n",
    "        self.log_sigma_cls = nn.Parameter(torch.tensor(0.0))\n",
    "        self.log_sigma_reg = nn.Parameter(torch.tensor(0.0))\n",
    "        self.alpha = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x).squeeze(2)\n",
    "        x = self.shared(x)\n",
    "        return self.classifier(x), self.regressor(x).squeeze(1)\n",
    "\n",
    "def souncertainty_weighted_loss(loss_cls, loss_reg, model):\n",
    "    return (\n",
    "        0.5 * loss_cls / torch.exp(2 * model.log_sigma_cls) + model.alpha * model.log_sigma_cls +\n",
    "        0.5 * loss_reg / torch.exp(2 * model.log_sigma_reg) + model.alpha * model.log_sigma_reg\n",
    "    )\n",
    "\n",
    "# === Train one model per cluster ===\n",
    "for cluster_id in range(n_clusters):\n",
    "    indices = np.where(train_clusters == cluster_id)[0]\n",
    "    X_c = X_train_scaled[indices]\n",
    "    y_cls_c = y_cls_train.iloc[indices]\n",
    "    y_reg_c = y_reg_train.iloc[indices]\n",
    "\n",
    "    dataset = TabularDataset(X_c, y_cls_c, y_reg_c)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = CNNMTLNetWithSOUW(input_dim=X_c.shape[1], num_classes=y_cls_train.nunique()).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_cls_fn = nn.CrossEntropyLoss()\n",
    "    loss_reg_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        for xb, yb_cls, yb_reg in loader:\n",
    "            xb, yb_cls, yb_reg = xb.to(device), yb_cls.to(device), yb_reg.to(device)\n",
    "            logits, reg_out = model(xb)\n",
    "            loss = souncertainty_weighted_loss(loss_cls_fn(logits, yb_cls), loss_reg_fn(reg_out, yb_reg), model)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    cluster_models[cluster_id] = model\n",
    "\n",
    "# === Predict for test data using nearest cluster ===\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "y_cls_preds, y_reg_preds = [], []\n",
    "\n",
    "for i in range(len(X_test_scaled)):\n",
    "    x_raw = torch.tensor(X_test_scaled[i], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    distances = euclidean_distances(X_test_scaled[i].reshape(1, -1), centroids)\n",
    "    cluster_id = np.argmin(distances)\n",
    "\n",
    "    model = cluster_models[cluster_id]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, reg_out = model(x_raw)\n",
    "        y_cls_preds.append(torch.argmax(logits, dim=1).item())\n",
    "        y_reg_preds.append(reg_out.item())\n",
    "\n",
    "# === Evaluate ===\n",
    "acc = accuracy_score(y_cls_test, y_cls_preds)\n",
    "r2 = r2_score(y_reg_test, y_reg_preds)\n",
    "\n",
    "print(f\"✅ Cluster-CNN MTL Accuracy (Species): {acc:.4f}\")\n",
    "print(f\"✅ Cluster-CNN MTL R² (Productivity): {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention - simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expt 1 - Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy (Species): 0.7760\n",
      "✅ R² Score (Productivity): 0.7731\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Load CSVs ===\n",
    "train_df = pd.read_csv(\"../../data/tr_data.csv\")\n",
    "test_df = pd.read_csv(\"../../data/te_data.csv\")\n",
    "\n",
    "drop_cols = ['Unnamed: 0', 'TestId', 'date_initial', 'date_final', 'Feature','latitude','longitute','env']\n",
    "train_df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "test_df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "\n",
    "# Drop NaNs\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Encode species\n",
    "le = LabelEncoder()\n",
    "train_df['species_encoded'] = le.fit_transform(train_df['Specie'])\n",
    "test_df['species_encoded'] = le.transform(test_df['Specie'])\n",
    "\n",
    "# Separate features and targets\n",
    "X_train = train_df.drop(columns=['Specie', 'species_encoded', 'Productivity (y)'])\n",
    "X_test = test_df.drop(columns=['Specie', 'species_encoded', 'Productivity (y)'])\n",
    "\n",
    "y_cls_train = train_df['species_encoded']\n",
    "y_reg_train = train_df['Productivity (y)']\n",
    "y_cls_test = test_df['species_encoded']\n",
    "y_reg_test = test_df['Productivity (y)']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Clustering ===\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "train_clusters = kmeans.fit_predict(X_train_scaled)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# === Dataset ===\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y_cls, y_reg):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_cls = torch.tensor(y_cls.values, dtype=torch.long)\n",
    "        self.y_reg = torch.tensor(y_reg.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y_cls[idx], self.y_reg[idx]\n",
    "\n",
    "# === Model ===\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.attn(x)  # (B, T, 1)\n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        pooled = torch.sum(weights * x, dim=1)  # (B, D)\n",
    "        return pooled\n",
    "\n",
    "class MLPTransformerMultiTask(nn.Module):\n",
    "    def __init__(self, input_dim=41, mlp_hidden_dim=128, transformer_dim=128,\n",
    "                 num_layers=2, num_loops=4, num_species=10, use_attention_pooling=True):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, transformer_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.num_loops = num_loops\n",
    "        self.pool = AttentionPooling(transformer_dim) if use_attention_pooling else lambda x: x.mean(dim=1)\n",
    "\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_species)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x_embed = self.mlp(x)  # (B, D)\n",
    "        x_seq = x_embed.unsqueeze(1).repeat(1, self.num_loops, 1)  # (B, T, D)\n",
    "        x_transformed = self.transformer(x_seq)  # (B, T, D)\n",
    "        x_pooled = self.pool(x_transformed)  # (B, D)\n",
    "        reg_out = self.reg_head(x_pooled).squeeze(1)\n",
    "        cls_logits = self.cls_head(x_pooled)\n",
    "        return cls_logits, reg_out\n",
    "\n",
    "# === Training one model per cluster ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cluster_models = {}\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    indices = np.where(train_clusters == cluster_id)[0]\n",
    "    X_cluster = X_train_scaled[indices]\n",
    "    y_cluster_cls = y_cls_train.iloc[indices]\n",
    "    y_cluster_reg = y_reg_train.iloc[indices]\n",
    "\n",
    "    train_loader = DataLoader(TabularDataset(X_cluster, y_cluster_cls, y_cluster_reg), batch_size=64, shuffle=True)\n",
    "\n",
    "    model = MLPTransformerMultiTask(\n",
    "        input_dim=X_cluster.shape[1],\n",
    "        num_species=y_cls_train.nunique(),\n",
    "        use_attention_pooling=True\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_reg = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        for xb, yb_cls, yb_reg in train_loader:\n",
    "            xb, yb_cls, yb_reg = xb.to(device), yb_cls.to(device), yb_reg.to(device)\n",
    "            logits, reg_out = model(xb)\n",
    "            loss = criterion_cls(logits, yb_cls) + criterion_reg(reg_out, yb_reg)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    cluster_models[cluster_id] = model\n",
    "\n",
    "# === Predict test samples using nearest cluster model ===\n",
    "y_cls_preds = []\n",
    "y_reg_preds = []\n",
    "\n",
    "for i in range(len(X_test_scaled)):\n",
    "    x = torch.tensor(X_test_scaled[i], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    distances = euclidean_distances(X_test_scaled[i].reshape(1, -1), centroids)\n",
    "    nearest_cluster = np.argmin(distances)\n",
    "\n",
    "    model = cluster_models[nearest_cluster]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, reg_out = model(x)\n",
    "        pred_cls = torch.argmax(logits, dim=1).item()\n",
    "        pred_reg = reg_out.item()\n",
    "\n",
    "    y_cls_preds.append(pred_cls)\n",
    "    y_reg_preds.append(pred_reg)\n",
    "\n",
    "# === Evaluate ===\n",
    "acc = accuracy_score(y_cls_test, y_cls_preds)\n",
    "r2 = r2_score(y_reg_test, y_reg_preds)\n",
    "\n",
    "print(f\"✅ Accuracy (Species): {acc:.4f}\")\n",
    "print(f\"✅ R² Score (Productivity): {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expt 2 - no clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tiles/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy (Species): 0.7792\n",
      "✅ R² Score (Productivity): 0.7465\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Load CSVs ===\n",
    "train_df = pd.read_csv(\"../../data/tr_data.csv\")\n",
    "test_df = pd.read_csv(\"../../data/te_data.csv\")\n",
    "\n",
    "drop_cols = ['Unnamed: 0', 'TestId', 'date_initial', 'date_final', 'Feature', 'latitude', 'longitute', 'env']\n",
    "train_df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "test_df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "\n",
    "# Drop NaNs\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Encode species\n",
    "le = LabelEncoder()\n",
    "train_df['species_encoded'] = le.fit_transform(train_df['Specie'])\n",
    "test_df['species_encoded'] = le.transform(test_df['Specie'])\n",
    "\n",
    "# Separate features and targets\n",
    "X_train = train_df.drop(columns=['Specie', 'species_encoded', 'Productivity (y)'])\n",
    "X_test = test_df.drop(columns=['Specie', 'species_encoded', 'Productivity (y)'])\n",
    "\n",
    "y_cls_train = train_df['species_encoded']\n",
    "y_reg_train = train_df['Productivity (y)']\n",
    "y_cls_test = test_df['species_encoded']\n",
    "y_reg_test = test_df['Productivity (y)']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Dataset ===\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y_cls, y_reg):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_cls = torch.tensor(y_cls.values, dtype=torch.long)\n",
    "        self.y_reg = torch.tensor(y_reg.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y_cls[idx], self.y_reg[idx]\n",
    "\n",
    "train_loader = DataLoader(TabularDataset(X_train_scaled, y_cls_train, y_reg_train), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TabularDataset(X_test_scaled, y_cls_test, y_reg_test), batch_size=64, shuffle=False)\n",
    "\n",
    "# === Model ===\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.attn(x)  # (B, T, 1)\n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        pooled = torch.sum(weights * x, dim=1)  # (B, D)\n",
    "        return pooled\n",
    "\n",
    "class MLPTransformerMultiTask(nn.Module):\n",
    "    def __init__(self, input_dim=41, mlp_hidden_dim=128, transformer_dim=128,\n",
    "                 num_layers=2, num_loops=4, num_species=10, use_attention_pooling=True):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, transformer_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.num_loops = num_loops\n",
    "        self.pool = AttentionPooling(transformer_dim) if use_attention_pooling else lambda x: x.mean(dim=1)\n",
    "\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_species)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x_embed = self.mlp(x)  # (B, D)\n",
    "        x_seq = x_embed.unsqueeze(1).repeat(1, self.num_loops, 1)  # (B, T, D)\n",
    "        x_transformed = self.transformer(x_seq)  # (B, T, D)\n",
    "        x_pooled = self.pool(x_transformed)  # (B, D)\n",
    "        reg_out = self.reg_head(x_pooled).squeeze(1)\n",
    "        cls_logits = self.cls_head(x_pooled)\n",
    "        return cls_logits, reg_out\n",
    "\n",
    "# === Train single model ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLPTransformerMultiTask(\n",
    "    input_dim=X_train.shape[1],\n",
    "    num_species=y_cls_train.nunique(),\n",
    "    use_attention_pooling=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_reg = nn.MSELoss()\n",
    "\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    for xb, yb_cls, yb_reg in train_loader:\n",
    "        xb, yb_cls, yb_reg = xb.to(device), yb_cls.to(device), yb_reg.to(device)\n",
    "        logits, reg_out = model(xb)\n",
    "        loss = criterion_cls(logits, yb_cls) + criterion_reg(reg_out, yb_reg)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# === Evaluate ===\n",
    "model.eval()\n",
    "y_cls_preds = []\n",
    "y_reg_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _, _ in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits, reg_out = model(xb)\n",
    "        pred_cls = torch.argmax(logits, dim=1)\n",
    "        y_cls_preds.extend(pred_cls.cpu().numpy())\n",
    "        y_reg_preds.extend(reg_out.cpu().numpy())\n",
    "\n",
    "# === Final Metrics ===\n",
    "acc = accuracy_score(y_cls_test, y_cls_preds)\n",
    "r2 = r2_score(y_reg_test, y_reg_preds)\n",
    "\n",
    "print(f\"✅ Accuracy (Species): {acc:.4f}\")\n",
    "print(f\"✅ R² Score (Productivity): {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
